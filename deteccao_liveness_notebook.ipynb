{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04b_5nZIT_S9"
      },
      "source": [
        "# MBA FIAP Inteligência Artificial & Machine Learning\n",
        "\n",
        "## Visão Computacional: Análise de Imagens Médicas\n",
        "\n",
        "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**.\n",
        "\n",
        "\n",
        "## 1. Introdução\n",
        "\n",
        "Uma determinada fintech focada em consumidores finais pessoa física constataou um grande número de fraudes em transações bancárias.\n",
        "\n",
        "O setor de fraudes apontou que existem clientes que se queixaram de não contratar serviços específicos, como o crédito pessoal, e após isso transferir para outras contas desconhecidas.\n",
        "\n",
        "Após análises pelas equipes de segurança, os protocolos de utilização da senha foram realizados em conformidade, ou seja, cada cliente autenticou com sua própria senha de maneira regular.\n",
        "\n",
        "Em função disso, o banco precisa arcar com reembolsos e medidas de contenção para evitar processos judiciais, pois os clientes alegam terem sido invadidos por hackers ou algo parecido.\n",
        "\n",
        "Uma das formas de solucionar ou minimizar este problema é com a utilização de outras formas de autenticação, sobretudo em operações críticas, como a obtenção de crédito pessoal.\n",
        "\n",
        "Desta forma podemos implementar uma verificação de identidade com prova de vida (liveness), que utilize uma verificação e identificação facial.\n",
        "\n",
        "Caso o cliente não seja autenticado, ele será atendido por uma esteira dedicada e as evidências da não identificação serão encaminhadas para a área de IA para validação dos parâmetros e limiares para aperfeiçoamento do modelo.\n",
        "\n",
        "Será necessário construir:\n",
        "\n",
        "* Detector de faces\n",
        "* Identificação de faces (podendo ser um comparador entre um rosto de documento e outra da prova de vida)\n",
        "* Detecção de vivacidade (liveness) para evitar que um fraudador utilize uma foto estática.\n",
        "\n",
        "\n",
        ">Formas alternativas de prover a identificação e prova de vivacidade, além destas que foram solicitadas poderão ser submetidas.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://github.com/michelpf/fiap-ml-visao-computacional-detector-liveness/blob/master/notebook/imagens/liveness.jpg?raw=1\">\n",
        "</p>\n",
        "\n",
        "Imagem retirada do [Grunge](https://www.grunge.com/192826/company-testing-robocop-facial-recognition-software-with-us-police/).\n",
        "\n",
        "## 2. Instruções\n",
        "\n",
        "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas.\n",
        "\n",
        "Iremos constuir uma forma de validar se uma determinada imagem foi ou não adulterada e se trata de uma produção fraudade.\n",
        "\n",
        "Existem diversas formas de validar a vivacidade, e neste sentido conto com a criatividade de vocês dado que já dominam encontrar uma face numa imagem, aplicar marcos faciais e até mesmo construir uma rede neural convulacional.\n",
        "\n",
        "A abordagem mais simples é pela construção de uma rede neural com imagens de fotos de rostos de outras fotos e fotos de rostos sem modificações. Tal classificador deverá classificar se dada imagem possui vivacidade ou não com uma pontuação de probabilidade.\n",
        "\n",
        "Referências que abordam o tema para servir de inspiração:\n",
        "\n",
        "1. [PyImageSearch](https://pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/), Liveness detection with OpenCV;\n",
        "2. [Kickertech](https://kickertech.com/face-liveness-detection-via-opencv-and-tensorflow/), Liveness detection via OpenCV and Tensorflow.\n",
        "3. [Towards Data Science](https://towardsdatascience.com/real-time-face-liveness-detection-with-python-keras-and-opencv-c35dc70dafd3?gi=24f8e1b740f9), Real-time face liveness detection with Python, Keras and OpenCV.\n",
        "\n",
        "Este projeto poderá ser feita por grupos de até 4 pessoas.\n",
        "Caso este projeto seja substitutivo, deverá ser realizado por apenas uma pessoa.\n",
        "\n",
        "| Nome dos Integrantes     | RM            | Turma |\n",
        "| :----------------------- | :------------- | :-----: |\n",
        "| Integrante 1             | RM 12345      | XIA |\n",
        "| Integrante 2             | RM 12345      | XIA |\n",
        "| Integrante 3             | RM 12345      | XIA |\n",
        "| Integrante 4             | RM 12345      | XIA |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5u5_hcMT_TA"
      },
      "source": [
        "## 3. Abordagem e organização da solução do problema (2 pontos)\n",
        "\n",
        "Como o grupo pretende deteccar a prova de vivacidade de uma determinada imagem? Quais os passos e os building blocks deste processo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gravaremos dois vídeos de cada integrante do grupo: um original e outro simulado. Usaremos esses vídeos para detectar rostos em cada frame e salvá-los em duas pastas: \"real\" e \"falso\". Essas imagens serão utilizadas para treinar um modelo de rede neural. Após o treinamento, o modelo será capaz de analisar características das imagens e determinar se são reais ou falsas. Por fim, a rede neural poderá ser usada com a câmera do computador ou com o upload de fotos para detectar a autenticidade das imagens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjb68xIdT_TA"
      },
      "source": [
        "**Resposta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPnV_04YT_TB"
      },
      "source": [
        "## 4 Desenvolvimento da solução (5,5 pontos)\n",
        "\n",
        "Detalhe o passo-a-passo do algoritmo de deteção de vivacidade.\n",
        "Se optar pela construção e treinamento de um modelo de redes neurais convulucionais, apresente a arquitetura, prepare os dados de treinamento, realize o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.0 Importação de bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import imutils\n",
        "import time\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imutils import paths\n",
        "from imutils.video import VideoStream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJqas1v4T_TB"
      },
      "source": [
        "### 4.1 Organização de dados para treinamento de modelo de liveness (2 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGRMY4DAT_TC"
      },
      "outputs": [],
      "source": [
        "# Define o valor mínimo de confiança para considerar uma detecção de rosto\n",
        "confidence_value = 0.50\n",
        "\n",
        "# Carrega o modelo de detecção de rostos do Caffe\n",
        "net = cv2.dnn.readNetFromCaffe(\"./deploy.prototxt.txt\", \"./res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "# Lista todos os vídeos no diretório \"./videos/\"\n",
        "videos = os.listdir(\"./videos/\")\n",
        "for video in videos:\n",
        "    # Ignora a pasta \"processados\"\n",
        "    if(video == \"processados\"):\n",
        "        break\n",
        "    \n",
        "    # Extrai o nome e o tipo do vídeo a partir do nome do arquivo\n",
        "    infos = video.split(\".\")[0].split(\"_\")\n",
        "    nome = infos[0]\n",
        "    tipo = infos[1]\n",
        "\n",
        "    # Abre o vídeo para leitura\n",
        "    vs = cv2.VideoCapture(\"./videos/\" + video)\n",
        "    saved_path = \"./dataset/\" + tipo\n",
        "    read = 0\n",
        "    saved = 0\n",
        "\n",
        "    while True:\n",
        "        # Lê um frame do vídeo\n",
        "        (grabbed, frame) = vs.read()\n",
        "        \n",
        "        # Interrompe após ler 100 frames\n",
        "        if read >= 100:\n",
        "            break\n",
        "        read += 1\n",
        "        \n",
        "        # Obtém as dimensões do frame e prepara o blob para a detecção de rostos\n",
        "        (h, w) = frame.shape[:2]\n",
        "        blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
        "            (300, 300), (104.0, 177.0, 123.0))\n",
        "        \n",
        "        # Passa o blob pela rede e obtém as detecções de rostos\n",
        "        net.setInput(blob)\n",
        "        detections = net.forward()\n",
        "\n",
        "        # Se houver detecções\n",
        "        if len(detections) > 0:\n",
        "            # Pega o índice da detecção com maior confiança\n",
        "            i = np.argmax(detections[0, 0, :, 2])\n",
        "            confidence = detections[0, 0, i, 2]\n",
        "\n",
        "            # Se a confiança for maior que o valor mínimo definido\n",
        "            if confidence > confidence_value:\n",
        "                # Calcula as coordenadas da caixa delimitadora do rosto\n",
        "                box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "                (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "                \n",
        "                # Extrai o rosto da imagem\n",
        "                face = frame[startY:endY, startX:endX]\n",
        "                \n",
        "                # Define o caminho e salva a imagem do rosto no disco\n",
        "                p = os.path.sep.join([saved_path, \"{}_{}_{}.png\".format(nome, tipo, saved)])\n",
        "                cv2.imwrite(p, face)\n",
        "                saved += 1\n",
        "                print(\"[INFO] saved {} to disk\".format(p))\n",
        "    \n",
        "    # Libera o vídeo e fecha todas as janelas do OpenCV\n",
        "    vs.release()\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH_H5GNUT_TC"
      },
      "source": [
        "### 4.2 Treinamento de modelo de liveness (1,5 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build(width, height, depth, classes):\n",
        "\t# Inicializa o modelo sequencial e a forma de entrada\n",
        "\tmodel = Sequential()\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\t\n",
        "\t# Ajusta a forma de entrada se o formato da imagem for \"channels_first\"\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\tinputShape = (depth, height, width)\n",
        "\t\tchanDim = 1\n",
        "\t\t\n",
        "\t# Primeira camada de convolução com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Conv2D(16, (3, 3), padding=\"same\", input_shape=inputShape))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\n",
        "\t# Segunda camada de convolução com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\n",
        "\t# Primeira camada de pooling e dropout\n",
        "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\tmodel.add(Dropout(0.25))\n",
        "\t\n",
        "\t# Terceira camada de convolução com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\n",
        "\t# Quarta camada de convolução com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\n",
        "\t# Segunda camada de pooling e dropout\n",
        "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\tmodel.add(Dropout(0.25))\n",
        "\t\n",
        "\t# Camada de flatten para transformar os dados em um vetor 1D\n",
        "\tmodel.add(Flatten())\n",
        "\t\n",
        "\t# Camada densa com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Dense(64))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\t\n",
        "\t# Camada de saída com ativação softmax para classificação\n",
        "\tmodel.add(Dense(classes))\n",
        "\tmodel.add(Activation(\"softmax\"))\n",
        "\t\n",
        "\t# Retorna o modelo construído\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xmreG1zT_TD"
      },
      "outputs": [],
      "source": [
        "# Definindo o tamanho do batch e o número de épocas\n",
        "BS = 8\n",
        "EPOCHS = 50\n",
        "\n",
        "print(\"[INFO] loading images...\")\n",
        "\n",
        "# Carrega as imagens do diretório \"./dataset/\"\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(\"./dataset/\"))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Loop através dos caminhos das imagens\n",
        "for imagePath in imagePaths:\n",
        "\t# Extrai o rótulo da imagem a partir do caminho do arquivo\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\t\n",
        "\t# Carrega a imagem, redimensiona para 32x32 pixels e adiciona aos dados\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = cv2.resize(image, (32, 32))\n",
        "\tdata.append(image)\n",
        "\t\n",
        "\t# Converte o rótulo em \"Falso\" ou \"Real\"\n",
        "\tlabels.append(\"Falso\" if label == \"./dataset/falso\" else \"Real\")\n",
        "\n",
        "# Converte os dados em um array numpy e normaliza os valores dos pixels para o intervalo [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "\n",
        "# Codifica os rótulos como inteiros e, em seguida, converte para vetores one-hot\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "labels = to_categorical(labels, 2)\n",
        "\n",
        "# Divide os dados em conjuntos de treinamento e teste\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Inicializa o gerador de dados para aumentar os dados de treinamento\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2,\n",
        "                         height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "# Constrói o modelo usando a função definida anteriormente\n",
        "model = build(width=32, height=32, depth=3, classes=len(le.classes_))\n",
        "\n",
        "# Compila o modelo usando a função de perda \"binary_crossentropy\" e o otimizador \"adam\"\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
        "\n",
        "# Treina a rede neural utilizando os dados de treinamento aumentados e valida utilizando os dados de teste\n",
        "H = model.fit(x=aug.flow(trainX, trainY, batch_size=BS), validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAup_gbOT_TD"
      },
      "source": [
        "### 4.3 Métricas de desempenho do modelo (2 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grP66-wjT_TD"
      },
      "outputs": [],
      "source": [
        "print(\"[INFO] evaluating network...\")\n",
        "\n",
        "# Faz previsões sobre os dados de teste\n",
        "predictions = model.predict(x=testX, batch_size=BS)\n",
        "\n",
        "# Imprime um relatório de classificação com as métricas de avaliação\n",
        "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=le.classes_))\n",
        "\n",
        "# Salva o modelo treinado no disco\n",
        "print(\"[INFO] serializing network to liveness.keras...\")\n",
        "model.save(\"liveness.keras\")\n",
        "\n",
        "# Salva o codificador de rótulos no disco\n",
        "f = open(\"le.pickle\", \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuzqKLehT_TD"
      },
      "source": [
        "## 5 Teste Fim-a-Fim\n",
        "\n",
        "Simule a operação fim-a-fim, com uma imagem de entrada forjada (foto de foto de um rosto) e outra com uma imagem de rosto, exibindo o resultado da classificação e a pontuação de cada classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39lGqGXqT_TE"
      },
      "outputs": [],
      "source": [
        "# Carrega o detector de faces\n",
        "print(\"[INFO] loading face detector...\")\n",
        "net = cv2.dnn.readNetFromCaffe(\"./deploy.prototxt.txt\", \"./res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "# Carrega o modelo de detecção de autenticidade de faces\n",
        "print(\"[INFO] loading liveness detector...\")\n",
        "model = load_model(\"liveness.keras\")\n",
        "le = pickle.loads(open(\"le.pickle\", \"rb\").read())\n",
        "\n",
        "# Inicia a transmissão de vídeo da webcam\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=0).start()\n",
        "time.sleep(2.0)  # Espera 2 segundos para a câmera inicializar\n",
        "\n",
        "while True:\n",
        "    # Lê um frame da transmissão de vídeo\n",
        "    frame = vs.read()\n",
        "    frame = imutils.resize(frame, width=600)\n",
        "    (h, w) = frame.shape[:2]\n",
        "    \n",
        "    # Prepara o frame para detecção de rostos\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
        "                                 (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # Loop sobre as detecções de rostos\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # Filtra detecções fracas\n",
        "        if confidence > 0.50:\n",
        "            # Calcula as coordenadas da caixa delimitadora do rosto\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            startX = max(0, startX)\n",
        "            startY = max(0, startY)\n",
        "            endX = min(w, endX)\n",
        "            endY = min(h, endY)\n",
        "            \n",
        "            # Extrai o rosto da imagem\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "            face = cv2.resize(face, (32, 32))\n",
        "            face = face.astype(\"float\") / 255.0\n",
        "            face = img_to_array(face)\n",
        "            face = np.expand_dims(face, axis=0)\n",
        "            \n",
        "            # Faz a previsão de autenticidade do rosto\n",
        "            preds = model.predict(face)[0]\n",
        "            j = np.argmax(preds)\n",
        "            label = le.classes_[j]\n",
        "\n",
        "            # Se o rosto for considerado \"Real\", desenha uma caixa azul, senão, desenha uma caixa vermelha\n",
        "            label = \"{}: {:.4f}\".format(label, preds[j])\n",
        "            if label == \"Real\":\n",
        "                cv2.putText(frame, label, (startX, startY - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
        "                              (255, 0, 0), 2)\n",
        "            else:\n",
        "                cv2.putText(frame, label, (startX, startY - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
        "                              (0, 0, 255), 2)\n",
        "\n",
        "    # Mostra o frame com as caixas delimitadoras e rótulos\n",
        "    cv2.imshow(\"Frame\", frame)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "    # Se a tecla 'q' for pressionada, sai do loop\n",
        "    if key == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "# Limpa as janelas e para a transmissão de vídeo\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzUPSL4RT_TE"
      },
      "source": [
        ">Com a implementação da solução na forma de uma aplicação do [Streamlit](https://www.streamlit.io/) (veja a pata streamlit-app e use o template) vale 1 ponto adicional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdILUumT_TE"
      },
      "source": [
        "**Pergunta**: Se utilizou o Streamlit, compartilhe a URL do aplicativo publicado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xatsk-r3T_TE"
      },
      "source": [
        "**Resposta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zZXU8WXT_TE"
      },
      "source": [
        "## 6 Conclusões (2,5 pontos)\n",
        "\n",
        "**Pergunta**: Dado todo o estudo e pesquisa, quais foram as conclusões sobre a solução, o que funcionou, o que não funcionou e quais os detalhes que observariam numa nova versão e melhorias do processo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69iIW5PwT_TE"
      },
      "source": [
        "**Resposta**:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "733a071da2455ea0e8bdf5409a7097e630ac701195faf55c6e985d77ee3ec176"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
