{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04b_5nZIT_S9"
      },
      "source": [
        "# MBA FIAP Inteligência Artificial & Machine Learning\n",
        "\n",
        "## Visão Computacional: Análise de Imagens Médicas\n",
        "\n",
        "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**.\n",
        "\n",
        "\n",
        "## 1. Introdução\n",
        "\n",
        "Uma determinada fintech focada em consumidores finais pessoa física constataou um grande número de fraudes em transações bancárias.\n",
        "\n",
        "O setor de fraudes apontou que existem clientes que se queixaram de não contratar serviços específicos, como o crédito pessoal, e após isso transferir para outras contas desconhecidas.\n",
        "\n",
        "Após análises pelas equipes de segurança, os protocolos de utilização da senha foram realizados em conformidade, ou seja, cada cliente autenticou com sua própria senha de maneira regular.\n",
        "\n",
        "Em função disso, o banco precisa arcar com reembolsos e medidas de contenção para evitar processos judiciais, pois os clientes alegam terem sido invadidos por hackers ou algo parecido.\n",
        "\n",
        "Uma das formas de solucionar ou minimizar este problema é com a utilização de outras formas de autenticação, sobretudo em operações críticas, como a obtenção de crédito pessoal.\n",
        "\n",
        "Desta forma podemos implementar uma verificação de identidade com prova de vida (liveness), que utilize uma verificação e identificação facial.\n",
        "\n",
        "Caso o cliente não seja autenticado, ele será atendido por uma esteira dedicada e as evidências da não identificação serão encaminhadas para a área de IA para validação dos parâmetros e limiares para aperfeiçoamento do modelo.\n",
        "\n",
        "Será necessário construir:\n",
        "\n",
        "* Detector de faces\n",
        "* Identificação de faces (podendo ser um comparador entre um rosto de documento e outra da prova de vida)\n",
        "* Detecção de vivacidade (liveness) para evitar que um fraudador utilize uma foto estática.\n",
        "\n",
        "\n",
        ">Formas alternativas de prover a identificação e prova de vivacidade, além destas que foram solicitadas poderão ser submetidas.\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "    <img src=\"https://github.com/michelpf/fiap-ml-visao-computacional-detector-liveness/blob/master/notebook/imagens/liveness.jpg?raw=1\">\n",
        "</p>\n",
        "\n",
        "Imagem retirada do [Grunge](https://www.grunge.com/192826/company-testing-robocop-facial-recognition-software-with-us-police/).\n",
        "\n",
        "## 2. Instruções\n",
        "\n",
        "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas.\n",
        "\n",
        "Iremos constuir uma forma de validar se uma determinada imagem foi ou não adulterada e se trata de uma produção fraudade.\n",
        "\n",
        "Existem diversas formas de validar a vivacidade, e neste sentido conto com a criatividade de vocês dado que já dominam encontrar uma face numa imagem, aplicar marcos faciais e até mesmo construir uma rede neural convulacional.\n",
        "\n",
        "A abordagem mais simples é pela construção de uma rede neural com imagens de fotos de rostos de outras fotos e fotos de rostos sem modificações. Tal classificador deverá classificar se dada imagem possui vivacidade ou não com uma pontuação de probabilidade.\n",
        "\n",
        "Referências que abordam o tema para servir de inspiração:\n",
        "\n",
        "1. [PyImageSearch](https://pyimagesearch.com/2019/03/11/liveness-detection-with-opencv/), Liveness detection with OpenCV;\n",
        "2. [Kickertech](https://kickertech.com/face-liveness-detection-via-opencv-and-tensorflow/), Liveness detection via OpenCV and Tensorflow.\n",
        "3. [Towards Data Science](https://towardsdatascience.com/real-time-face-liveness-detection-with-python-keras-and-opencv-c35dc70dafd3?gi=24f8e1b740f9), Real-time face liveness detection with Python, Keras and OpenCV.\n",
        "\n",
        "Este projeto poderá ser feita por grupos de até 4 pessoas.\n",
        "Caso este projeto seja substitutivo, deverá ser realizado por apenas uma pessoa.\n",
        "\n",
        "| Nome dos Integrantes     | RM            | Turma |\n",
        "| :----------------------- | :------------- | :-----: |\n",
        "| Integrante 1             | RM 12345      | XIA |\n",
        "| Integrante 2             | RM 12345      | XIA |\n",
        "| Integrante 3             | RM 12345      | XIA |\n",
        "| Integrante 4             | RM 12345      | XIA |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5u5_hcMT_TA"
      },
      "source": [
        "## 3. Abordagem e organização da solução do problema (2 pontos)\n",
        "\n",
        "Como o grupo pretende deteccar a prova de vivacidade de uma determinada imagem? Quais os passos e os building blocks deste processo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Gravaremos dois vídeos de cada integrante do grupo: um original e outro simulado. Usaremos esses vídeos para detectar rostos em cada frame e salvá-los em duas pastas: \"real\" e \"falso\". Essas imagens serão utilizadas para treinar um modelo de rede neural. Após o treinamento, o modelo será capaz de analisar características das imagens e determinar se são reais ou falsas. Por fim, a rede neural poderá ser usada com a câmera do computador ou com o upload de fotos para detectar a autenticidade das imagens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjb68xIdT_TA"
      },
      "source": [
        "**Resposta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPnV_04YT_TB"
      },
      "source": [
        "## 4 Desenvolvimento da solução (5,5 pontos)\n",
        "\n",
        "Detalhe o passo-a-passo do algoritmo de deteção de vivacidade.\n",
        "Se optar pela construção e treinamento de um modelo de redes neurais convulucionais, apresente a arquitetura, prepare os dados de treinamento, realize o treinamento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.0 Importação de bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-07-07 04:45:52.177762: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-07-07 04:45:52.210376: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-07-07 04:45:52.210410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-07-07 04:45:52.211413: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-07-07 04:45:52.216862: I external/local_tsl/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-07-07 04:45:52.217690: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2024-07-07 04:45:53.170483: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import pickle\n",
        "import imutils\n",
        "import time\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from imutils import paths\n",
        "from imutils.video import VideoStream"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJqas1v4T_TB"
      },
      "source": [
        "### 4.1 Organização de dados para treinamento de modelo de liveness (2 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "AGRMY4DAT_TC"
      },
      "outputs": [],
      "source": [
        "# # Define o valor mínimo de confiança para considerar uma detecção de rosto\n",
        "# confidence_value = 0.50\n",
        "\n",
        "# # Carrega o modelo de detecção de rostos do Caffe\n",
        "# net = cv2.dnn.readNetFromCaffe(\"./deploy.prototxt.txt\", \"./res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "# # Lista todos os vídeos no diretório \"./videos/\"\n",
        "# videos = os.listdir(\"./videos/\")\n",
        "# for video in videos:\n",
        "#     # Ignora a pasta \"processados\"\n",
        "#     if(video == \"processados\"):\n",
        "#         break\n",
        "    \n",
        "#     # Extrai o nome e o tipo do vídeo a partir do nome do arquivo\n",
        "#     infos = video.split(\".\")[0].split(\"_\")\n",
        "#     nome = infos[0]\n",
        "#     tipo = infos[1]\n",
        "\n",
        "#     # Abre o vídeo para leitura\n",
        "#     vs = cv2.VideoCapture(\"./videos/\" + video)\n",
        "#     saved_path = \"./dataset/\" + tipo\n",
        "#     read = 0\n",
        "#     saved = 0\n",
        "\n",
        "#     while True:\n",
        "#         # Lê um frame do vídeo\n",
        "#         (grabbed, frame) = vs.read()\n",
        "        \n",
        "#         # Interrompe após ler 100 frames\n",
        "#         if read >= 100:\n",
        "#             break\n",
        "#         read += 1\n",
        "        \n",
        "#         # Obtém as dimensões do frame e prepara o blob para a detecção de rostos\n",
        "#         (h, w) = frame.shape[:2]\n",
        "#         blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
        "#             (300, 300), (104.0, 177.0, 123.0))\n",
        "        \n",
        "#         # Passa o blob pela rede e obtém as detecções de rostos\n",
        "#         net.setInput(blob)\n",
        "#         detections = net.forward()\n",
        "\n",
        "#         # Se houver detecções\n",
        "#         if len(detections) > 0:\n",
        "#             # Pega o índice da detecção com maior confiança\n",
        "#             i = np.argmax(detections[0, 0, :, 2])\n",
        "#             confidence = detections[0, 0, i, 2]\n",
        "\n",
        "#             # Se a confiança for maior que o valor mínimo definido\n",
        "#             if confidence > confidence_value:\n",
        "#                 # Calcula as coordenadas da caixa delimitadora do rosto\n",
        "#                 box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "#                 (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "                \n",
        "#                 # Extrai o rosto da imagem\n",
        "#                 face = frame[startY:endY, startX:endX]\n",
        "                \n",
        "#                 # Define o caminho e salva a imagem do rosto no disco\n",
        "#                 p = os.path.sep.join([saved_path, \"{}_{}_{}.png\".format(nome, tipo, saved)])\n",
        "#                 cv2.imwrite(p, face)\n",
        "#                 saved += 1\n",
        "#                 print(\"[INFO] saved {} to disk\".format(p))\n",
        "    \n",
        "#     # Libera o vídeo e fecha todas as janelas do OpenCV\n",
        "#     vs.release()\n",
        "# cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH_H5GNUT_TC"
      },
      "source": [
        "### 4.2 Treinamento de modelo de liveness (1,5 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build(width, height, depth, classes):\n",
        "\t# Inicializa o modelo sequencial e a forma de entrada\n",
        "\tmodel = Sequential()\n",
        "\tinputShape = (height, width, depth)\n",
        "\tchanDim = -1\n",
        "\t\n",
        "\t# Ajusta a forma de entrada se o formato da imagem for \"channels_first\"\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\tinputShape = (depth, height, width)\n",
        "\t\tchanDim = 1\n",
        "\t\t\n",
        "\t# Primeira camada de convolução com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Conv2D(16, (3, 3), padding=\"same\", input_shape=inputShape))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\n",
        "\t# Segunda camada de convolução com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\n",
        "\t# Primeira camada de pooling e dropout\n",
        "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\tmodel.add(Dropout(0.25))\n",
        "\t\n",
        "\t# Terceira camada de convolução com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\n",
        "\t# Quarta camada de convolução com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization(axis=chanDim))\n",
        "\t\n",
        "\t# Segunda camada de pooling e dropout\n",
        "\tmodel.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\tmodel.add(Dropout(0.25))\n",
        "\t\n",
        "\t# Camada de flatten para transformar os dados em um vetor 1D\n",
        "\tmodel.add(Flatten())\n",
        "\t\n",
        "\t# Camada densa com ativação ReLU e normalização em lote\n",
        "\tmodel.add(Dense(64))\n",
        "\tmodel.add(Activation(\"relu\"))\n",
        "\tmodel.add(BatchNormalization())\n",
        "\tmodel.add(Dropout(0.5))\n",
        "\t\n",
        "\t# Camada de saída com ativação softmax para classificação\n",
        "\tmodel.add(Dense(classes))\n",
        "\tmodel.add(Activation(\"softmax\"))\n",
        "\t\n",
        "\t# Retorna o modelo construído\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_xmreG1zT_TD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading images...\n",
            "[INFO] loading images...\n",
            "[INFO] compiling model...\n",
            "[INFO] training network for 50 epochs...\n",
            "Epoch 1/50\n",
            "75/75 [==============================] - 3s 22ms/step - loss: 0.3421 - accuracy: 0.8667 - val_loss: 0.7901 - val_accuracy: 0.4900\n",
            "Epoch 2/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1993 - accuracy: 0.9333 - val_loss: 0.8867 - val_accuracy: 0.4900\n",
            "Epoch 3/50\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.1389 - accuracy: 0.9650 - val_loss: 0.6848 - val_accuracy: 0.8400\n",
            "Epoch 4/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0907 - accuracy: 0.9683 - val_loss: 0.3908 - val_accuracy: 0.8900\n",
            "Epoch 5/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1427 - accuracy: 0.9483 - val_loss: 0.0532 - val_accuracy: 1.0000\n",
            "Epoch 6/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0918 - accuracy: 0.9700 - val_loss: 0.0892 - val_accuracy: 1.0000\n",
            "Epoch 7/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0953 - accuracy: 0.9633 - val_loss: 0.0587 - val_accuracy: 0.9800\n",
            "Epoch 8/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.1327 - accuracy: 0.9583 - val_loss: 0.0258 - val_accuracy: 0.9950\n",
            "Epoch 9/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1076 - accuracy: 0.9533 - val_loss: 0.0266 - val_accuracy: 1.0000\n",
            "Epoch 10/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1009 - accuracy: 0.9633 - val_loss: 0.0087 - val_accuracy: 1.0000\n",
            "Epoch 11/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1364 - accuracy: 0.9483 - val_loss: 0.0161 - val_accuracy: 1.0000\n",
            "Epoch 12/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0887 - accuracy: 0.9717 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
            "Epoch 13/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0866 - accuracy: 0.9750 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
            "Epoch 14/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0776 - accuracy: 0.9783 - val_loss: 0.0077 - val_accuracy: 1.0000\n",
            "Epoch 15/50\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.0942 - accuracy: 0.9750 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 16/50\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.1392 - accuracy: 0.9483 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
            "Epoch 17/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1089 - accuracy: 0.9650 - val_loss: 0.2458 - val_accuracy: 0.8600\n",
            "Epoch 18/50\n",
            "75/75 [==============================] - 2s 22ms/step - loss: 0.0494 - accuracy: 0.9800 - val_loss: 0.5832 - val_accuracy: 0.7900\n",
            "Epoch 19/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0861 - accuracy: 0.9750 - val_loss: 4.5155e-04 - val_accuracy: 1.0000\n",
            "Epoch 20/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0972 - accuracy: 0.9667 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
            "Epoch 21/50\n",
            "75/75 [==============================] - 2s 21ms/step - loss: 0.0614 - accuracy: 0.9800 - val_loss: 5.3674e-04 - val_accuracy: 1.0000\n",
            "Epoch 22/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0868 - accuracy: 0.9767 - val_loss: 0.0990 - val_accuracy: 0.9400\n",
            "Epoch 23/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0966 - accuracy: 0.9717 - val_loss: 0.1147 - val_accuracy: 0.9600\n",
            "Epoch 24/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.1235 - accuracy: 0.9650 - val_loss: 0.0045 - val_accuracy: 1.0000\n",
            "Epoch 25/50\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.0870 - accuracy: 0.9733 - val_loss: 0.0108 - val_accuracy: 1.0000\n",
            "Epoch 26/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0555 - accuracy: 0.9767 - val_loss: 0.0079 - val_accuracy: 1.0000\n",
            "Epoch 27/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0856 - accuracy: 0.9750 - val_loss: 0.0614 - val_accuracy: 0.9800\n",
            "Epoch 28/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0613 - accuracy: 0.9750 - val_loss: 0.0289 - val_accuracy: 0.9900\n",
            "Epoch 29/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0769 - accuracy: 0.9733 - val_loss: 0.0170 - val_accuracy: 1.0000\n",
            "Epoch 30/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0599 - accuracy: 0.9833 - val_loss: 3.3223e-04 - val_accuracy: 1.0000\n",
            "Epoch 31/50\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.1212 - accuracy: 0.9683 - val_loss: 0.0456 - val_accuracy: 0.9850\n",
            "Epoch 32/50\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.0399 - accuracy: 0.9850 - val_loss: 3.1724e-04 - val_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0242 - accuracy: 0.9950 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
            "Epoch 34/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0729 - accuracy: 0.9700 - val_loss: 8.0655e-05 - val_accuracy: 1.0000\n",
            "Epoch 35/50\n",
            "75/75 [==============================] - 2s 20ms/step - loss: 0.0705 - accuracy: 0.9733 - val_loss: 8.0927e-05 - val_accuracy: 1.0000\n",
            "Epoch 36/50\n",
            "75/75 [==============================] - 1s 17ms/step - loss: 0.0418 - accuracy: 0.9850 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
            "Epoch 37/50\n",
            "75/75 [==============================] - 1s 20ms/step - loss: 0.0786 - accuracy: 0.9767 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
            "Epoch 38/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1025 - accuracy: 0.9667 - val_loss: 6.7326e-04 - val_accuracy: 1.0000\n",
            "Epoch 39/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0486 - accuracy: 0.9883 - val_loss: 0.0097 - val_accuracy: 0.9950\n",
            "Epoch 40/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0604 - accuracy: 0.9783 - val_loss: 0.0329 - val_accuracy: 0.9850\n",
            "Epoch 41/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0391 - accuracy: 0.9867 - val_loss: 4.5409e-04 - val_accuracy: 1.0000\n",
            "Epoch 42/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0452 - accuracy: 0.9867 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0348 - accuracy: 0.9933 - val_loss: 1.1100e-04 - val_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0579 - accuracy: 0.9800 - val_loss: 1.0449e-04 - val_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.0654 - accuracy: 0.9817 - val_loss: 0.0053 - val_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1052 - accuracy: 0.9600 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "75/75 [==============================] - 1s 19ms/step - loss: 0.1364 - accuracy: 0.9567 - val_loss: 7.2843e-04 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "75/75 [==============================] - 2s 23ms/step - loss: 0.1246 - accuracy: 0.9617 - val_loss: 0.0139 - val_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.1383 - accuracy: 0.9533 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
            "Epoch 50/50\n",
            "75/75 [==============================] - 1s 18ms/step - loss: 0.0894 - accuracy: 0.9683 - val_loss: 9.3836e-04 - val_accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Definindo o tamanho do batch e o número de épocas\n",
        "BS = 8\n",
        "EPOCHS = 50\n",
        "\n",
        "print(\"[INFO] loading images...\")\n",
        "\n",
        "# Carrega as imagens do diretório \"./dataset/\"\n",
        "print(\"[INFO] loading images...\")\n",
        "imagePaths = list(paths.list_images(\"./dataset/\"))\n",
        "data = []\n",
        "labels = []\n",
        "\n",
        "# Loop através dos caminhos das imagens\n",
        "for imagePath in imagePaths:\n",
        "\t# Extrai o rótulo da imagem a partir do caminho do arquivo\n",
        "\tlabel = imagePath.split(os.path.sep)[-2]\n",
        "\t\n",
        "\t# Carrega a imagem, redimensiona para 32x32 pixels e adiciona aos dados\n",
        "\timage = cv2.imread(imagePath)\n",
        "\timage = cv2.resize(image, (32, 32))\n",
        "\tdata.append(image)\n",
        "\t\n",
        "\t# Adiciona o rótulo \"Falso\" ou \"Real\"\n",
        "\tlabels.append(label)\n",
        "\n",
        "# Converte os dados em um array numpy e normaliza os valores dos pixels para o intervalo [0, 1]\n",
        "data = np.array(data, dtype=\"float\") / 255.0\n",
        "\n",
        "# Codifica os rótulos como inteiros e, em seguida, converte para vetores one-hot\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "labels = to_categorical(labels, 2)\n",
        "\n",
        "# Divide os dados em conjuntos de treinamento e teste\n",
        "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Inicializa o gerador de dados para aumentar os dados de treinamento\n",
        "aug = ImageDataGenerator(rotation_range=20, zoom_range=0.15, width_shift_range=0.2,\n",
        "                         height_shift_range=0.2, shear_range=0.15, horizontal_flip=True, fill_mode=\"nearest\")\n",
        "\n",
        "print(\"[INFO] compiling model...\")\n",
        "\n",
        "# Constrói o modelo usando a função definida anteriormente\n",
        "model = build(width=32, height=32, depth=3, classes=len(le.classes_))\n",
        "\n",
        "# Compila o modelo usando a função de perda \"binary_crossentropy\" e o otimizador \"adam\"\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "print(\"[INFO] training network for {} epochs...\".format(EPOCHS))\n",
        "\n",
        "# Treina a rede neural utilizando os dados de treinamento aumentados e valida utilizando os dados de teste\n",
        "H = model.fit(x=aug.flow(trainX, trainY, batch_size=BS), validation_data=(testX, testY), steps_per_epoch=len(trainX) // BS, epochs=EPOCHS)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAup_gbOT_TD"
      },
      "source": [
        "### 4.3 Métricas de desempenho do modelo (2 pontos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "grP66-wjT_TD"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] evaluating network...\n",
            "25/25 [==============================] - 0s 4ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       falso       1.00      1.00      1.00       102\n",
            "        real       1.00      1.00      1.00        98\n",
            "\n",
            "    accuracy                           1.00       200\n",
            "   macro avg       1.00      1.00      1.00       200\n",
            "weighted avg       1.00      1.00      1.00       200\n",
            "\n",
            "[INFO] serializing network to liveness.keras...\n"
          ]
        }
      ],
      "source": [
        "print(\"[INFO] evaluating network...\")\n",
        "\n",
        "# Faz previsões sobre os dados de teste\n",
        "predictions = model.predict(x=testX, batch_size=BS)\n",
        "\n",
        "# Imprime um relatório de classificação com as métricas de avaliação\n",
        "print(classification_report(testY.argmax(axis=1), predictions.argmax(axis=1), target_names=le.classes_))\n",
        "\n",
        "# Salva o modelo treinado no disco\n",
        "print(\"[INFO] serializing network to liveness.keras...\")\n",
        "model.save(\"liveness.keras\")\n",
        "\n",
        "# Salva o codificador de rótulos no disco\n",
        "f = open(\"le.pickle\", \"wb\")\n",
        "f.write(pickle.dumps(le))\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuzqKLehT_TD"
      },
      "source": [
        "## 5 Teste Fim-a-Fim\n",
        "\n",
        "Simule a operação fim-a-fim, com uma imagem de entrada forjada (foto de foto de um rosto) e outra com uma imagem de rosto, exibindo o resultado da classificação e a pontuação de cada classe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "39lGqGXqT_TE"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[INFO] loading face detector...\n",
            "[INFO] loading liveness detector...\n",
            "[INFO] starting video stream...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[ WARN:0@119.438] global cap_v4l.cpp:982 open VIDEOIO(V4L2:/dev/video0): can't open camera by index\n",
            "[ERROR:0@119.449] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'shape'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Lê um frame da transmissão de vídeo\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     frame \u001b[38;5;241m=\u001b[39m vs\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 18\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[43mimutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     (h, w) \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Prepara o frame para detecção de rostos\u001b[39;00m\n",
            "File \u001b[0;32m/workspaces/deteccao_liveness/.venv/lib/python3.11/site-packages/imutils/convenience.py:69\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(image, width, height, inter)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize\u001b[39m(image, width\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, height\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, inter\u001b[38;5;241m=\u001b[39mcv2\u001b[38;5;241m.\u001b[39mINTER_AREA):\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# initialize the dimensions of the image to be resized and\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;66;03m# grab the image size\u001b[39;00m\n\u001b[1;32m     68\u001b[0m     dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m     (h, w) \u001b[38;5;241m=\u001b[39m \u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# if both the width and height are None, then return the\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# original image\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m width \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m height \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
          ]
        }
      ],
      "source": [
        "# Carrega o detector de faces\n",
        "print(\"[INFO] loading face detector...\")\n",
        "net = cv2.dnn.readNetFromCaffe(\"./deploy.prototxt.txt\", \"./res10_300x300_ssd_iter_140000.caffemodel\")\n",
        "\n",
        "# Carrega o modelo de detecção de autenticidade de faces\n",
        "print(\"[INFO] loading liveness detector...\")\n",
        "model = load_model(\"liveness.keras\")\n",
        "le = pickle.loads(open(\"le.pickle\", \"rb\").read())\n",
        "\n",
        "# Inicia a transmissão de vídeo da webcam\n",
        "print(\"[INFO] starting video stream...\")\n",
        "vs = VideoStream(src=0).start()\n",
        "time.sleep(2.0)  # Espera 2 segundos para a câmera inicializar\n",
        "\n",
        "while True:\n",
        "    # Lê um frame da transmissão de vídeo\n",
        "    frame = vs.read()\n",
        "    frame = imutils.resize(frame, width=600)\n",
        "    (h, w) = frame.shape[:2]\n",
        "    \n",
        "    # Prepara o frame para detecção de rostos\n",
        "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
        "                                 (300, 300), (104.0, 177.0, 123.0))\n",
        "    net.setInput(blob)\n",
        "    detections = net.forward()\n",
        "\n",
        "    # Loop sobre as detecções de rostos\n",
        "    for i in range(0, detections.shape[2]):\n",
        "        confidence = detections[0, 0, i, 2]\n",
        "\n",
        "        # Filtra detecções fracas\n",
        "        if confidence > 0.50:\n",
        "            # Calcula as coordenadas da caixa delimitadora do rosto\n",
        "            box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
        "            (startX, startY, endX, endY) = box.astype(\"int\")\n",
        "            startX = max(0, startX)\n",
        "            startY = max(0, startY)\n",
        "            endX = min(w, endX)\n",
        "            endY = min(h, endY)\n",
        "            \n",
        "            # Extrai o rosto da imagem\n",
        "            face = frame[startY:endY, startX:endX]\n",
        "            face = cv2.resize(face, (32, 32))\n",
        "            face = face.astype(\"float\") / 255.0\n",
        "            face = img_to_array(face)\n",
        "            face = np.expand_dims(face, axis=0)\n",
        "            \n",
        "            # Faz a previsão de autenticidade do rosto\n",
        "            preds = model.predict(face)[0]\n",
        "            j = np.argmax(preds)\n",
        "            label = le.classes_[j]\n",
        "\n",
        "            # Se o rosto for considerado \"Real\", desenha uma caixa azul, senão, desenha uma caixa vermelha\n",
        "            label = \"{}: {:.4f}\".format(label, preds[j])\n",
        "            if label == \"Real\":\n",
        "                cv2.putText(frame, label, (startX, startY - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
        "                              (255, 0, 0), 2)\n",
        "            else:\n",
        "                cv2.putText(frame, label, (startX, startY - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 2)\n",
        "                cv2.rectangle(frame, (startX, startY), (endX, endY),\n",
        "                              (0, 0, 255), 2)\n",
        "\n",
        "    # Mostra o frame com as caixas delimitadoras e rótulos\n",
        "    cv2.imshow(\"Frame\", frame)\n",
        "    key = cv2.waitKey(1) & 0xFF\n",
        "\n",
        "    # Se a tecla 'q' for pressionada, sai do loop\n",
        "    if key == ord(\"q\"):\n",
        "        break\n",
        "\n",
        "# Limpa as janelas e para a transmissão de vídeo\n",
        "cv2.destroyAllWindows()\n",
        "vs.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OzUPSL4RT_TE"
      },
      "source": [
        ">Com a implementação da solução na forma de uma aplicação do [Streamlit](https://www.streamlit.io/) (veja a pata streamlit-app e use o template) vale 1 ponto adicional."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RdILUumT_TE"
      },
      "source": [
        "**Pergunta**: Se utilizou o Streamlit, compartilhe a URL do aplicativo publicado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xatsk-r3T_TE"
      },
      "source": [
        "**Resposta**:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zZXU8WXT_TE"
      },
      "source": [
        "## 6 Conclusões (2,5 pontos)\n",
        "\n",
        "**Pergunta**: Dado todo o estudo e pesquisa, quais foram as conclusões sobre a solução, o que funcionou, o que não funcionou e quais os detalhes que observariam numa nova versão e melhorias do processo?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69iIW5PwT_TE"
      },
      "source": [
        "**Resposta**:"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.13 ('.venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "733a071da2455ea0e8bdf5409a7097e630ac701195faf55c6e985d77ee3ec176"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
